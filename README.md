# Neo4j CDC Sync on Azure

Real-time replication between two Neo4j Aura instances using Change Data Capture (CDC), Azure Event Hubs, and Kafka Connect.

```
Neo4j Aura (master, CDC-enabled)
    → Neo4j CDC Source Connector
        → Azure Event Hubs (Kafka API)
            → Neo4j CDC Sink Connector
                → Neo4j Aura (subscriber)
```

Kafka Connect runs on Azure Container Instance alongside a heartbeat sidecar that keeps Event Hubs connections alive. The Docker images are built and pushed to Azure Container Registry as part of the deploy.

All resources are co-located in the same Azure region as the Aura instances.

## Prerequisites

- [Terraform](https://developer.hashicorp.com/terraform/install) >= 1.5
- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) — authenticated via `az login`
- [Neo4j Aura API credentials](https://console.neo4j.io) — Account > API Keys > Create New API Key
- [Conda](https://docs.conda.io/en/latest/miniconda.html) or [Mamba](https://mamba.readthedocs.io/) — for Python environment

**Note:** Docker is NOT required. Container images are built in Azure using `az acr build`.

## Setup

Create and activate the conda environment:

```bash
conda env create -f environment.yml
conda activate neo4j-cdc-sync
```

This installs Python 3.11 and all required dependencies (neo4j driver, requests, rich).

## ⚠️ Security Warning

**This deployment exposes the Kafka Connect REST API (port 8083) publicly to the internet.**

### Acceptable Use
- ✅ Short-term demos and learning (< 1 day)
- ✅ Environments where you run `terraform destroy` immediately after testing

### NOT Acceptable
- ❌ Production systems
- ❌ Long-running deployments
- ❌ Systems handling sensitive data

### Why This Matters

The Kafka Connect REST API at port 8083 allows anyone who finds your IP to:
- Deploy malicious connectors to your cluster
- Read Event Hubs connection strings from connector configurations
- Potentially access your Neo4j instances if credentials are exposed in configs

### Mitigation

1. **Recommended for Demos**: Run `terraform destroy` immediately after your demo
2. **Basic Protection**: Implement IP whitelisting (requires ACI configuration changes)
3. **Production Setup**: Use Azure VNet integration with private endpoints (requires Premium Event Hubs)

**Bottom line:** This is safe for 1-day demos, but NOT for production or long-running systems.

## Deploy

```bash
# 1. Activate conda environment
conda activate neo4j-cdc-sync

# 2. Authenticate with Azure
az login

# 3. Configure terraform
cd terraform
cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars and add your Aura API credentials
# Get credentials from: https://console.neo4j.io → Account → API Keys

# 4. Optional: run pre-flight checks
python ../python/preflight.py

# 5. Deploy
terraform init
terraform apply
```

### Deployment Time

- Initial deployment: ~8-10 minutes
- Subsequent deployments (cached): ~3-4 minutes

Terraform provisions two Aura instances, an Event Hubs namespace with single-partition topic, a container registry, builds the Docker images in Azure, and starts a container group with Kafka Connect + heartbeat sidecar. The source and sink connectors are deployed automatically.

## Get credentials

Passwords are generated by Aura during provisioning. Terraform stores them in state and exposes them as outputs:

```bash
cd terraform

# Show everything (URIs, usernames, Event Hubs, Kafka Connect IP)
terraform output

# Show passwords (sensitive outputs require the -raw flag)
terraform output -raw master_neo4j_password
terraform output -raw subscriber_neo4j_password
```

## Verify

Check that both connectors are running:

```bash
KAFKA_CONNECT=$(terraform output -raw kafka_connect_rest_api)

curl -s $KAFKA_CONNECT/connectors/neo4j-master-publisher/status | jq '.connector.state, .tasks[0].state'
curl -s $KAFKA_CONNECT/connectors/neo4j-subscriber-consumer/status | jq '.connector.state, .tasks[0].state'
```

Both should report `"RUNNING"`.

Run the CDC test script to verify CREATE, UPDATE, and DELETE events propagate:

```bash
cd terraform
export MASTER_NEO4J_URI=$(terraform output -raw master_neo4j_uri)
export MASTER_NEO4J_PASSWORD=$(terraform output -raw master_neo4j_password)
export SUBSCRIBER_NEO4J_URI=$(terraform output -raw subscriber_neo4j_uri)
export SUBSCRIBER_NEO4J_PASSWORD=$(terraform output -raw subscriber_neo4j_password)
cd ../python
python test_live_cdc.py
```

This tests all CDC event types and reports actual propagation latency (typically 1-2 seconds).

## Demo flow (for presenting to users)

Use this sequence when demonstrating CDC to an audience.

**1. Set the scene**

- Two Neo4j Aura instances: a **master** (source of truth) and a **subscriber** (replica).
- Changes on the master are streamed in real time to the subscriber via CDC → Event Hubs → Kafka Connect.
- No polling or batch jobs; each write is captured and replicated as an event.

**2. Show the pipeline is running**

```bash
cd terraform
KAFKA_CONNECT=$(terraform output -raw kafka_connect_rest_api)
curl -s $KAFKA_CONNECT/connectors | jq .
curl -s $KAFKA_CONNECT/connectors/neo4j-master-publisher/status | jq '.connector.state, .tasks[0].state'
curl -s $KAFKA_CONNECT/connectors/neo4j-subscriber-consumer/status | jq '.connector.state, .tasks[0].state'
```

Both connectors should be `"RUNNING"`.

**3. Run the automated CDC test**

```bash
cd terraform
export MASTER_NEO4J_URI=$(terraform output -raw master_neo4j_uri)
export MASTER_NEO4J_PASSWORD=$(terraform output -raw master_neo4j_password)
export SUBSCRIBER_NEO4J_URI=$(terraform output -raw subscriber_neo4j_uri)
export SUBSCRIBER_NEO4J_PASSWORD=$(terraform output -raw subscriber_neo4j_password)
cd ../python
python test_live_cdc.py
```

- The heartbeat sidecar keeps the pipeline warm, so tests start immediately — no warm-up delay.
- CREATE, UPDATE, and DELETE are run in sequence. Each step shows propagation time (typically under 2 seconds).

**4. Optional: show it live in the Aura consoles**

- Open the **master** instance in [Neo4j Aura Console](https://console.neo4j.io) and run a write, e.g. `CREATE (d:Demo {name: 'Live demo', at: datetime()})`.
- Open the **subscriber** instance and run `MATCH (d:Demo) RETURN d`. After a second or two, the node appears (with a `SourceEvent` label added by the sink).

**5. Wrap up**

- Emphasize: same graph model, real-time sync, no application code changes on the master; the subscriber is a read replica fed by CDC.
- Tear down when done: `cd terraform && terraform destroy` to avoid ongoing cost.

## Tear down

```bash
cd terraform
terraform destroy
```

This removes all Azure and Aura resources. Cost while running is roughly $255/month ($8.50/day), so destroy after demos.

## Project structure

```
terraform/
  main.tf                     # Root module — orchestrates everything
  variables.tf                # All configurable parameters
  outputs.tf                  # Connection URIs, passwords, endpoints
  providers.tf                # AzureRM and Neo4j Aura providers
  terraform.tfvars.example    # Template for your credentials
  modules/
    aura-instance/            # Neo4j Aura provisioning
    event-hubs/               # Azure Event Hubs namespace + topics
    acr/                      # Container registry + Docker build/push
    aci/                      # Container instance + connector deployment
  scripts/
    configure_connectors.py   # Connector configuration (called by local-exec)

kafka-connect/
  Dockerfile                  # Custom image with Neo4j Connector 5.2.0

heartbeat/
  heartbeat.py                # Keeps Event Hubs connections alive
  Dockerfile                  # Lightweight Python sidecar image

python/                       # Verification and test scripts
generators/                   # Sample data generators
```

## Secret scanning

This repo uses [gitleaks](https://github.com/gitleaks/gitleaks) to prevent accidentally committing secrets. Install the pre-commit hook before contributing:

```bash
brew install gitleaks pre-commit   # macOS
pre-commit install
```

Secrets are blocked at commit time. To scan manually:

```bash
gitleaks detect --source . --verbose
```

The `.gitignore` excludes `.env`, `*.tfvars`, `terraform.tfstate*`, and generated connector JSON files.

## Troubleshooting

### Pre-Deployment Issues

**Error: "Azure CLI not authenticated"**
- Run: `az login`
- Verify: `az account show`

**Error: "Python dependencies missing"**
- Activate environment: `conda activate neo4j-cdc-sync`
- Or create it: `conda env create -f environment.yml`
- Verify: `python3 -c "import rich, neo4j, requests"`

**Error: "terraform.tfvars not found or incomplete"**
- Copy template: `cp terraform/terraform.tfvars.example terraform/terraform.tfvars`
- Edit and add your Aura API credentials from https://console.neo4j.io

### Connector Issues

**Connector shows FAILED**

Check detailed error:
```bash
KAFKA_CONNECT=$(terraform output -raw kafka_connect_rest_api)
curl -s $KAFKA_CONNECT/connectors/neo4j-master-publisher/tasks/0/status | jq '.trace'
curl -s $KAFKA_CONNECT/connectors/neo4j-subscriber-consumer/tasks/0/status | jq '.trace'
```

Restart failed connector:
```bash
curl -X POST $KAFKA_CONNECT/connectors/neo4j-master-publisher/tasks/0/restart
```

**Source connector stuck in "STARTING"**

Event Hubs metadata timeout (known issue). Restart the task:
```bash
curl -X POST $KAFKA_CONNECT/connectors/neo4j-master-publisher/tasks/0/restart
```

The connector deployment script automatically restarts once, but you may need to restart manually if idle for a while.

### CDC Sync Issues

**Relationships missing in subscriber (nodes replicate but relationships don't)**

This is the **multi-partition issue** — the most common CDC failure mode.

**Cause:** Event Hubs topic has >1 partition, breaking CDC ordering. Relationships arrive before their nodes, causing foreign key violations.

**Fix:**
```bash
# Option A: Delete topic in Azure Portal, then re-apply
# Azure Portal → Event Hubs → Your namespace → Event Hubs → cdc-all → Delete
cd terraform
terraform apply

# Option B: Destroy and recreate everything
cd terraform
terraform destroy
terraform apply
```

**Verify fix:**
```bash
cd python
python verify_cdc.py
```

**Nothing replicating (target shows 0 nodes)**

The heartbeat sidecar keeps connections alive, so this should not happen under normal operation. If it does:
- Check the heartbeat container logs: Azure Portal → Container Group → Containers → heartbeat → Logs
- If the heartbeat container is not running, restart the container group: `terraform apply`
- As a quick fix, restart connectors manually:

```bash
curl -X POST $KAFKA_CONNECT/connectors/neo4j-master-publisher/tasks/0/restart
curl -X POST $KAFKA_CONNECT/connectors/neo4j-subscriber-consumer/tasks/0/restart
```

**Slow propagation (>5 seconds per event)**

- On first deploy, Event Hubs needs 1-2 minutes to establish connections
- The heartbeat sidecar keeps connections warm after that — sustained latency should be 1-2 seconds
- Check connectors are RUNNING: `curl $KAFKA_CONNECT/connectors/neo4j-master-publisher/status`

**Sink connector error: "Node not found"**

This means relationships arrived before nodes (ordering violation):
- Verify topic has exactly 1 partition (see multi-partition fix above)
- Check source connector config has `topic.creation.default.partitions=1` (should be automatic)
- Restart connectors and clear data: `terraform destroy && terraform apply`

### Other Issues

**Aura region error: "Invalid region"**

The Aura provider expects region names like `eastus`, not `azure-eastus`. The `azure_region` variable in `variables.tf` is passed directly to both Azure and Aura resources.

**Docker image OS mismatch on Apple Silicon**

The image is now built in Azure using `az acr build`, which always produces linux/amd64 images compatible with Azure Container Instances.

**Python dependencies not found during Terraform run**

Make sure the conda environment is activated BEFORE running Terraform:
```bash
conda activate neo4j-cdc-sync
cd terraform
terraform apply
```

You can run `python python/preflight.py` first to verify all prerequisites are met.
