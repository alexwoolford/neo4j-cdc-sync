# Neo4j CDC Sync on Azure

Real-time replication between two Neo4j Aura instances using Change Data Capture (CDC), Azure Event Hubs, and Kafka Connect.

```
Neo4j Aura (master, CDC-enabled)
    → Neo4j CDC Source Connector
        → Azure Event Hubs (Kafka API)
            → Neo4j CDC Sink Connector
                → Neo4j Aura (subscriber)
```

Kafka Connect runs on Azure Container Instance. The Docker image (with Neo4j Connector 5.2.0) is built and pushed to Azure Container Registry as part of the deploy.

All resources are co-located in the same Azure region as the Aura instances.

## Prerequisites

- [Terraform](https://developer.hashicorp.com/terraform/install) >= 1.5
- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) — authenticated via `az login`
- [Docker](https://docs.docker.com/get-docker/) — for building the Kafka Connect image
- [Neo4j Aura API credentials](https://console.neo4j.io) — Account > API Keys > Create New API Key
- [Conda](https://docs.conda.io/en/latest/miniconda.html) or [Mamba](https://mamba.readthedocs.io/) — for Python environment

## Setup

Create and activate the conda environment:

```bash
conda env create -f environment.yml
conda activate neo4j-cdc-sync
```

This installs Python 3.11 and all required dependencies (neo4j driver, requests, rich).

## Deploy

```bash
cd terraform
cp terraform.tfvars.example terraform.tfvars
```

Edit `terraform.tfvars` and fill in your Aura API credentials:

```hcl
aura_client_id     = "your-client-id"
aura_client_secret = "your-client-secret"
aura_tenant_id     = "your-tenant-id"
```

Then deploy:

```bash
terraform init
terraform apply
```

This takes about 8 minutes. Terraform provisions two Aura instances, an Event Hubs namespace, a container registry, builds and pushes the Docker image, starts a Kafka Connect container, and deploys the source and sink connectors.

## Get credentials

Passwords are generated by Aura during provisioning. Terraform stores them in state and exposes them as outputs:

```bash
cd terraform

# Show everything (URIs, usernames, Event Hubs, Kafka Connect IP)
terraform output

# Show passwords (sensitive outputs require the -raw flag)
terraform output -raw master_neo4j_password
terraform output -raw subscriber_neo4j_password
```

## Verify

Check that both connectors are running:

```bash
KAFKA_CONNECT=$(terraform output -raw kafka_connect_rest_api)

curl -s $KAFKA_CONNECT/connectors/neo4j-master-publisher/status | jq '.connector.state, .tasks[0].state'
curl -s $KAFKA_CONNECT/connectors/neo4j-subscriber-consumer/status | jq '.connector.state, .tasks[0].state'
```

Both should report `"RUNNING"`.

Run the CDC test script to verify CREATE, UPDATE, and DELETE events propagate:

```bash
cd terraform
export MASTER_NEO4J_URI=$(terraform output -raw master_neo4j_uri)
export MASTER_NEO4J_PASSWORD=$(terraform output -raw master_neo4j_password)
export SUBSCRIBER_NEO4J_URI=$(terraform output -raw subscriber_neo4j_uri)
export SUBSCRIBER_NEO4J_PASSWORD=$(terraform output -raw subscriber_neo4j_password)
cd ../python
python test_live_cdc.py
```

This tests all CDC event types and reports actual propagation latency (typically 1-2 seconds).

## Demo flow (for presenting to users)

Use this sequence when demonstrating CDC to an audience.

**1. Set the scene**

- Two Neo4j Aura instances: a **master** (source of truth) and a **subscriber** (replica).
- Changes on the master are streamed in real time to the subscriber via CDC → Event Hubs → Kafka Connect.
- No polling or batch jobs; each write is captured and replicated as an event.

**2. Show the pipeline is running**

```bash
cd terraform
KAFKA_CONNECT=$(terraform output -raw kafka_connect_rest_api)
curl -s $KAFKA_CONNECT/connectors | jq .
curl -s $KAFKA_CONNECT/connectors/neo4j-master-publisher/status | jq '.connector.state, .tasks[0].state'
curl -s $KAFKA_CONNECT/connectors/neo4j-subscriber-consumer/status | jq '.connector.state, .tasks[0].state'
```

Both connectors should be `"RUNNING"`.

**3. Run the automated CDC test**

```bash
cd terraform
export MASTER_NEO4J_URI=$(terraform output -raw master_neo4j_uri)
export MASTER_NEO4J_PASSWORD=$(terraform output -raw master_neo4j_password)
export SUBSCRIBER_NEO4J_URI=$(terraform output -raw subscriber_neo4j_uri)
export SUBSCRIBER_NEO4J_PASSWORD=$(terraform output -raw subscriber_neo4j_password)
cd ../python
python test_live_cdc.py
```

- **Warm-up**: If the pipeline has been idle, the script waits for Event Hubs connections to come back (up to ~2 min). Tell the audience this is expected with Azure Event Hubs after idle.
- **Tests**: CREATE, UPDATE, and DELETE are run in sequence. Each step shows propagation time (typically under 2 seconds).

**4. Optional: show it live in the Aura consoles**

- Open the **master** instance in [Neo4j Aura Console](https://console.neo4j.io) and run a write, e.g. `CREATE (d:Demo {name: 'Live demo', at: datetime()})`.
- Open the **subscriber** instance and run `MATCH (d:Demo) RETURN d`. After a second or two, the node appears (with a `SourceEvent` label added by the sink).

**5. Wrap up**

- Emphasize: same graph model, real-time sync, no application code changes on the master; the subscriber is a read replica fed by CDC.
- Tear down when done: `cd terraform && terraform destroy` to avoid ongoing cost.

## Tear down

```bash
cd terraform
terraform destroy
```

This removes all Azure and Aura resources. Cost while running is roughly $255/month ($8.50/day), so destroy after demos.

## Project structure

```
terraform/
  main.tf                     # Root module — orchestrates everything
  variables.tf                # All configurable parameters
  outputs.tf                  # Connection URIs, passwords, endpoints
  providers.tf                # AzureRM and Neo4j Aura providers
  terraform.tfvars.example    # Template for your credentials
  modules/
    aura-instance/            # Neo4j Aura provisioning
    event-hubs/               # Azure Event Hubs namespace + topics
    acr/                      # Container registry + Docker build/push
    aci/                      # Container instance + connector deployment

kafka-connect/
  Dockerfile                  # Custom image with Neo4j Connector 5.2.0

python/                       # Optional verification and test scripts
generators/                   # Optional sample data generators
```

## Secret scanning

This repo uses [gitleaks](https://github.com/gitleaks/gitleaks) to prevent accidentally committing secrets. Install the pre-commit hook before contributing:

```bash
brew install gitleaks pre-commit   # macOS
pre-commit install
```

Secrets are blocked at commit time. To scan manually:

```bash
gitleaks detect --source . --verbose
```

The `.gitignore` excludes `.env`, `*.tfvars`, `terraform.tfstate*`, and generated connector JSON files.

## Troubleshooting

**Connector shows FAILED**: Check the task trace for details:

```bash
curl -s $KAFKA_CONNECT/connectors/neo4j-master-publisher/tasks/0/status | jq '.trace'
```

To restart a failed task:

```bash
curl -X POST $KAFKA_CONNECT/connectors/neo4j-master-publisher/tasks/0/restart
```

Note: The deploy script automatically restarts the source task after initial deployment to work around an Azure Event Hubs metadata timeout issue.

**Aura region error**: The Aura provider expects region names like `eastus`, not `azure-eastus`. The `azure_region` variable in `variables.tf` is passed directly to both Azure and Aura resources.

**Docker image OS mismatch on Apple Silicon**: The Dockerfile builds with `--platform linux/amd64` to target Azure Container Instances.

**Python dependencies not found**: Make sure the conda environment is activated before running terraform:

```bash
conda activate neo4j-cdc-sync
```
